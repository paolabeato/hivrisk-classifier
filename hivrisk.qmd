---
title: "Predicting HIV Risk from Behavioral Data: A Machine Learning Workflow in R"
author: "Paola Beato Fernández"
format: html
editor: visual
toc: true
---

```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
library(conflicted)
suppressPackageStartupMessages(library(tidymodels))
tidymodels_prefer()
suppressPackageStartupMessages(library(tidyverse))
options(dplyr.summarise.inform = FALSE)
suppressPackageStartupMessages(library(gtsummary)) 
library(glue)
library(rUM)
suppressMessages(library(rio)) 
library(table1)
library(gt)
library(readr)
library(kableExtra)
library(labelled)
library(forcats)
library(moments)
library(officer)
library(purrr)
library(janitor)
library(ggplot2)
library(lme4)
library(broom)
library(quarto)
library(skimr)
library(correlation)
library(DT)
library(plotly)
library(ggpubr)
library(FSelectorRcpp)
library(yardstick)
library(vip)
```

```{r import, echo=FALSE}
hiv_risk <- suppressMessages(
  read_csv("/Users/paolabeato/Desktop/PBF/Job Apps 2025/Portfolio/HIV Risk/HIV_RISK_SIMULATED.csv", show_col_types = FALSE)
)
```

```{r cleaning, echo=FALSE}
# Set seed for reproducibility of probabilistic risk assignment
set.seed(2025)

# Define clean and labeled dataset with probabilistic HIV risk logic
hiv_risk_clean <- hiv_risk %>%
  mutate(
    # Factor conversions (demographics)
    sex = factor(sex, levels = c(1, 2, 3), labels = c("Male", "Female", "Other")),
    education = factor(education, levels = c(1, 2, 3, 4, 5), 
                       labels = c("Less than HS", "High School", "Some College", "Bachelor", "Graduate")),
    race = factor(race, levels = c(1, 2, 3, 4), labels = c("Black", "White", "Latinx", "Other")),
    condom_use = factor(condom_use, levels = c(1, 2, 3), labels = c("Always", "Sometimes", "Never")),
    
    # Factor conversions (binary)
    drug_use = factor(drug_use, levels = c(0, 1), labels = c("No", "Yes")),
    transactional_sex = factor(transactional_sex, levels = c(0, 1), labels = c("No", "Yes")),
    recent_sti = factor(recent_sti, levels = c(0, 1), labels = c("No", "Yes")),
    prep_awareness = factor(prep_awareness, levels = c(0, 1), labels = c("No", "Yes")),

    # Numeric HIV risk outcome using probabilistic logic
    hiv_risk = case_when(
      transactional_sex == "Yes" & recent_sti == "Yes" & runif(n()) < 0.9 ~ 1,
      drug_use == "Yes" & condom_use == "Never" & runif(n()) < 0.9 ~ 1,
      num_partners > 5 & condom_use == "Never" & runif(n()) < 0.85 ~ 1,
      recent_sti == "Yes" & num_partners > 3 & runif(n()) < 0.8 ~ 1,
      transactional_sex == "Yes" & runif(n()) < 0.7 ~ 1,
      num_partners > 3 & condom_use == "Sometimes" & runif(n()) < 0.5 ~ 1,
      drug_use == "Yes" & runif(n()) < 0.3 ~ 1,
      TRUE ~ 0
    ),

    # Convert to factor with labels
    hiv_risk = factor(hiv_risk, levels = c(0, 1), labels = c("Low", "High"))
  )

# Adding a label for gtsummary output
attr(hiv_risk_clean$hiv_risk, "label") <- "Predicted HIV Risk Group"
attr(hiv_risk_clean$sex, "label") <- "Sex"
attr(hiv_risk_clean$race, "label") <- "Race"
attr(hiv_risk_clean$age, "label") <- "Age (years)"
attr(hiv_risk_clean$education, "label") <- "Educational Attainment"
attr(hiv_risk_clean$num_partners, "label") <- "Number of Sexual Partners in the Past 6 Months"
attr(hiv_risk_clean$condom_use, "label") <- "Condom Use in the Past 6 Months"
attr(hiv_risk_clean$drug_use, "label") <- "Drug Use in the Past 6 Months"
attr(hiv_risk_clean$transactional_sex, "label") <- "Transactional Sex in the Past Year"
attr(hiv_risk_clean$recent_sti, "label") <- "STI Diagnosis in the Past 12 months"
attr(hiv_risk_clean$prep_awareness, "label") <- "PreP Awareness"
```

# Introduction

This project demonstrates how machine learning techniques can be applied to behavioral and sociodemographic data to predict HIV risk. Using a simulated dataset of 5,000 individuals, we explore the relationship between known risk-related factors—such as condom use, drug use, and recent STI diagnosis—and the likelihood of being classified as high-risk. The goal is to build reproducible and interpretable predictive models while showcasing the end-to-end modeling pipeline: from data cleaning and exploratory analysis to feature selection and model evaluation.

# Methods

### Data Simulation & Cleaning

The dataset used in this project was synthetically generated to resemble a behavioral surveillance dataset typical of public health research. It includes variables on demographics (sex, age, race, education), sexual behavior (number of partners, condom use), and clinical history (drug use, STI diagnosis, PrEP awareness).

The binary HIV risk outcome was derived using a probabilistic logic tree based on these characteristics, designed to reflect plausible epidemiological associations.

Categorical variables were recoded into descriptive factor levels, and variable labels were added to support interpretability and tidy outputs. The final cleaned dataset contained 5,000 observations.

### Exploratory Data Analysis (EDA)

EDA was conducted to assess variable distributions and identify group differences between individuals classified as low versus high HIV risk. Summary tables were created using the `gtsummary` package, and visualizations were generated with `ggplot2`. Key behavioral predictors—such as condom use, number of sexual partners, drug use, and STI history—showed clear separation between risk groups.

### Modeling Approach

Three classification models were developed using the `tidymodels` framework:

-   Logistic Regression — a baseline, interpretable model

-   Random Forest — a nonlinear, ensemble-based model

-   Tuned Random Forest — optimized via cross-validated hyperparameter tuning

#### Data Preparation

The data was split into training (75%) and testing (25%) sets using stratified sampling to preserve HIV risk group proportions. A preprocessing recipe was applied to:

-   A preprocessing recipe was applied, including:
-   Dummy encoding of categorical predictors
-   Log transformation of skewed count variables (e.g., number of partners)
-   Removal of zero-variance predictors
-   Normalization of numeric features

#### Predictor Selection

To focus modeling on the most informative features, information gain was computed for each predictor. The top 8 variables—primarily behavioral factors such as transactional sex, drug use, and condom use—were selected for inclusion in the models. These aligned well with known HIV risk factors.

### Model Tuning and Evaluation

The Random Forest model was tuned using 5-fold cross-validation across a grid of `mtry` and `min_n` values, optimizing for ROC AUC.

All models were evaluated on the held-out test set using:

-   Accuracy and Cohen’s Kappa (overall classification performance)
-   Confusion matrices (Figures 6-9)
-   Variable importance plots (Figures 8 & 10)
-   A combined ROC curve (Figure 11) to visualize and compare discrimination performance across all three models.

The tuned random forest model achieved the highest ROC AUC and demonstrated superior balance in sensitivity and specificity, making it the strongest overall performer.

# Exploratory Data Analysis

```{r sociodemographics, echo=FALSE}
library(gtsummary)

demographic_table <- hiv_risk_clean %>%
  select(hiv_risk, sex, age, race, education, num_partners, condom_use,
         drug_use, transactional_sex, recent_sti, prep_awareness) %>%
  tbl_summary(
    by = hiv_risk,
    statistic = list(
      all_continuous() ~ "{mean} ({sd})",
      all_categorical() ~ "{n} ({p}%)"
    ),
    label = list(
      sex ~ "Sex",
      age ~ "Age",
      race ~ "Race/Ethnicity",
      education ~ "Educational Attainment",
      num_partners ~ "Number of Sexual Partners in the Past 6 Months",
      condom_use ~ "Condom Use in the Past 6 Months",
      drug_use ~ "Drug Use in the Past 6 Months",
      transactional_sex ~ "Transactional Sex in the Past Year",
      recent_sti ~ "STI Diagnosis in the Past 12 months",
      prep_awareness ~ "PrEP Awareness"
    )
  ) %>%
  add_p() %>%
  bold_labels() %>%
  modify_header(label = "**Variable**") %>%
  modify_caption("**Table 1. Sociodemographic and Behavioral Characteristics by HIV Risk Group**")

demographic_table
```

The table summarizes key demographic and behavioral predictors by HIV risk group. Most sociodemographic variables (sex, age, race, education) showed no statistically significant differences between risk groups (p \> 0.05), suggesting relatively balanced distribution across these characteristics.

However, behavioral factors were significantly associated with HIV risk:

-   Individuals in the high-risk group reported a greater number of sexual partners on average (3.7 vs. 2.7, *p* \< 0.001).

-   High-risk individuals were more likely to report never using condoms, engaging in drug use, participating in transactional sex, and having a recent STI (*p* \< 0.001 for all).

```{r figure1, echo=FALSE}
ggplot(hiv_risk_clean, aes(x = condom_use, fill = as.factor(hiv_risk))) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = scales::percent_format()) +
  scale_fill_manual(values = c("#4E79A7", "#F28E2B"),
                    name = "HIV Risk", labels = c("Low", "High")) +
  labs(title = "Figure 1: Condom Use by HIV Risk Group", x = "Condom Use", y = "Proportion") +
  theme_minimal()
```

Condom use behavior strongly differentiates risk groups:

-   Among those who never used condoms, nearly 40% were classified as high risk, compared to just \~20% among those who always used condoms.

-   Risk increases progressively from "Always" → "Sometimes" → "Never," reinforcing the importance of consistent condom use in HIV prevention.

```{r figure2, echo=FALSE}
ggplot(hiv_risk_clean, aes(x = as.factor(hiv_risk), y = age, fill = as.factor(hiv_risk))) +
  geom_boxplot(alpha = 0.7) +
  scale_fill_manual(values = c("#4E79A7", "#F28E2B"), guide = "none") +
  labs(title = "Figure 2: Age Distribution by HIV Risk Group", x = "HIV Risk", y = "Age") +
  theme_minimal()
```

There was no meaningful difference in age distribution between high and low HIV risk groups:

-   Median age was comparable (Low = 39, High = 38).

-   This suggests that age alone is not a strong differentiator of risk in this sample.

```{r figure3, echo=FALSE}
ggplot(hiv_risk_clean, aes(x = race, fill = as.factor(hiv_risk))) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = scales::percent_format()) +
  scale_fill_manual(values = c("#4E79A7", "#F28E2B"), name = "HIV Risk", labels = c("Low", "High")) +
  labs(title = "Figure 3: Race Distribution by HIV Risk", x = "Race/Ethnicity", y = "Proportion") +
  theme_minimal()
```

Race/ethnicity was not strongly associated with HIV risk in the simulated data. The proportion of individuals at high risk was relatively uniform across Black, White, Latinx, and Other racial groups.

```{r figure4, echo=FALSE}
library(tidyr)
library(forcats)

# Reshape data to long format for plotting
hiv_risk_long <- hiv_risk_clean %>%
  select(hiv_risk, drug_use, transactional_sex, recent_sti, prep_awareness) %>%
  pivot_longer(cols = -hiv_risk, names_to = "variable", values_to = "response") %>%
  mutate(variable = fct_recode(variable,
                               "Drug Use" = "drug_use",
                               "Transactional Sex" = "transactional_sex",
                               "Recent STI" = "recent_sti",
                               "PrEP Awareness" = "prep_awareness"))

# Plot faceted bar plots
ggplot(hiv_risk_long, aes(x = response, fill = as.factor(hiv_risk))) +
  geom_bar(position = "fill") +
  facet_wrap(~variable, scales = "free") +
  scale_y_continuous(labels = scales::percent_format()) +
  scale_fill_manual(values = c("#4E79A7", "#F28E2B"),
                    name = "HIV Risk", labels = c("Low", "High")) +
  labs(x = NULL, y = "Proportion", title = "Figure 4: Binary Predictors by HIV Risk Group") +
  theme_minimal(base_size = 12) +
  theme(legend.position = "top")
```

This faceted plot reinforces key behavioral distinctions:

-   Drug use and recent STI history were much more common in the high-risk group (\~55% vs. \~19% and \~31% vs. \~16%, respectively).

-   Transactional sex showed one of the starkest divides, with 26% of high-risk individuals engaging in it versus just 2% in the low-risk group.

-   PrEP awareness, in contrast, did not differ substantially between groups—highlighting a potential missed prevention opportunity.

```{r figure5, echo=FALSE}
# Age
p1 <- ggplot(hiv_risk_clean, aes(x = age, fill = as.factor(hiv_risk))) +
  geom_density(alpha = 0.5) +
  scale_fill_manual(values = c("#4E79A7", "#F28E2B")) +
  labs(title = "Age Distribution", fill = "HIV Risk") +
  theme_minimal()

# Number of Partners
p2 <- ggplot(hiv_risk_clean, aes(x = num_partners, fill = as.factor(hiv_risk))) +
  geom_histogram(alpha = 0.6, bins = 30, position = "identity") +
  scale_fill_manual(values = c("#4E79A7", "#F28E2B")) +
  labs(title = "Number of Partners", fill = "HIV Risk") +
  theme_minimal()

# Combine with ggpubr
combined_plot <- ggarrange(
  p1, p2,
  ncol = 2,
  common.legend = TRUE,
  legend = "bottom"
)

# Add a single title
annotate_figure(
  combined_plot,
  top = text_grob("Figure 5: Age and Number of Partners by HIV Risk Group", 
                  face = "bold", size = 14)
)
```

-   Age distributions again appear similar between groups, reinforcing earlier findings.

-   The distribution of number of sexual partners shows a clear rightward skew in the high-risk group, with more individuals reporting 5+ partners.

# Statistical Modeling

## Predictor Selection

To identify the most informative features for predicting HIV risk, information gain was computed for each variable. The top predictors included behavioral risk factors:

```{r predictor-selection, echo=FALSE}
# Step 1: Compute Information Gain
info_gain_tbl <- information_gain(hiv_risk ~ ., data = hiv_risk_clean)

# Step 2: Clean column names (optional)
info_gain_tbl <- info_gain_tbl %>%
  clean_names()  # Will result in: attributes, importance

# Step 3: Pick top predictors
top_n <- 8
top_predictors <- info_gain_tbl %>%
  arrange(desc(importance)) %>%
  slice_head(n = top_n) %>%
  pull(attributes)

# Step 4: Label map for readable table
label_map <- c(
  sex = "Sex",
  age = "Age",
  race = "Race/Ethnicity",
  education = "Education",
  num_partners = "Number of Partners",
  condom_use = "Condom Use",
  drug_use = "Drug Use",
  transactional_sex = "Transactional Sex",
  recent_sti = "Recent STI",
  prep_awareness = "PrEP Awareness"
)

# Step 5: Create a pretty table
info_gain_pretty <- info_gain_tbl %>%
  filter(attributes %in% top_predictors) %>%
  mutate(
    Variable = ifelse(attributes %in% names(label_map),
                      label_map[attributes],
                      attributes)
  ) %>%
  select(Variable, Importance = importance) %>%
  arrange(desc(Importance))

# Step 6: Display with gt
info_gain_pretty %>%
  gt() %>%
  tab_header(
    title = md("**Table 2. Top Predictors Based on Information Gain**")
  ) %>%
  fmt_number(columns = "Importance", decimals = 3)
```

-   Transactional sex and drug use showed the highest information gain, followed by number of partners, condom use, and recent STI.

-   Sociodemographic variables (e.g., race, education, PrEP awareness) contributed little to prediction in this synthetic dataset.

```{r model-setup, echo=FALSE}
set.seed(2025)

# Split the data
hiv_split <- initial_split(hiv_risk_clean, prop = 0.75, strata = hiv_risk)
hiv_train <- training(hiv_split)
hiv_test  <- testing(hiv_split)

# Filter top predictors to those that exist in the cleaned data
selected_predictors <- intersect(top_predictors, colnames(hiv_risk_clean))

# Define final variable set (predictors + outcome)
selected_vars <- c(selected_predictors, "hiv_risk")

# Subset train and test data
hiv_train_sel <- hiv_train %>% select(all_of(selected_vars))
hiv_test_sel  <- hiv_test %>% select(all_of(selected_vars))

logistic_model <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

rf_model <- rand_forest(trees = 500) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")
```

```{r recipe, echo=FALSE}
# Recipe using only selected predictors
hiv_recipe_sel <- recipe(hiv_risk ~ ., data = hiv_train_sel) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_log(num_partners, offset = 1) %>%  # optional
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric_predictors())
```

```{r workflows, echo=FALSE}
# Logistic workflow
logistic_wf_sel <- workflow() %>%
  add_model(logistic_model) %>%
  add_recipe(hiv_recipe_sel)

# RF workflow
rf_wf_sel <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(hiv_recipe_sel)
```

```{r fitting-models, echo=FALSE}
# Fit models with selected predictors
logistic_fit_sel <- fit(logistic_wf_sel, data = hiv_train_sel)
rf_fit_sel       <- fit(rf_wf_sel, data = hiv_train_sel)
```

## Logistic Regression Model

```{r logistic-model, echo=FALSE}

# Predictions
log_preds_sel <- predict(logistic_fit_sel, hiv_test_sel, type = "prob") %>%
  bind_cols(predict(logistic_fit_sel, hiv_test_sel), truth = hiv_test_sel$hiv_risk)

# Raw metrics
log_metrics_sel <- log_preds_sel %>%
  metrics(truth = truth, estimate = .pred_class)

# Format for display
log_metrics_pretty <- log_metrics_sel %>%
  mutate(
    Metric = case_when(
      .metric == "accuracy" ~ "Accuracy",
      .metric == "kap" ~ "Cohen's Kappa"
    ),
    Value = scales::percent(.estimate, accuracy = 0.1)
  ) %>%
  select(Metric, Value)

# Display with gt
log_metrics_pretty %>%
  gt() %>%
  tab_header(
    title = md("**Table 3. Logistic Regression Model Performance**")
  )

# ROC AUC for logistic
log_roc <- roc_curve(log_preds_sel, truth = truth, .pred_High) %>%
  mutate(model = "Logistic Regression")

log_auc <- roc_auc(log_preds_sel, truth = truth, .pred_High) %>%
  pull(.estimate)
```

The logistic regression model achieved an accuracy of 83.5% and a Cohen’s Kappa of 58.8%, indicating substantial agreement beyond chance in classifying individuals as high or low HIV risk. The model achieved an ROC AUC of `r scales::percent(log_auc, accuracy = 0.1)`, indicating good discriminative ability in distinguishing between low and high-risk individuals.

```{r figure6, echo=FALSE}
log_preds_sel %>%
  conf_mat(truth = truth, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  ggtitle("Figure 6: Confusion Matrix for Logistic Regression Model") +
  theme_minimal()
```

The confusion matrix provides more granular insight:

-   True Negatives (Low risk correctly classified): 800

-   False Positives (Low risk misclassified as high): 126

-   False Negatives (High risk misclassified as low): 81

-   True Positives (High risk correctly classified): 244

The model is slightly more likely to misclassify low-risk individuals as high-risk (Type I error) than to miss high-risk individuals (Type II error). This may be preferable in public health settings where false negatives (missed high-risk individuals) carry greater consequences than false positives.

Overall, the model demonstrates solid performance and could be a useful screening tool for identifying individuals at elevated HIV risk based on behavioral and demographic indicators.

## Untuned Random Forest Model

```{r untuned-rf-model, echo=FALSE}
# Predictions
rf_preds_sel <- predict(rf_fit_sel, hiv_test_sel, type = "prob") %>%
  bind_cols(predict(rf_fit_sel, hiv_test_sel), truth = hiv_test_sel$hiv_risk)

# Raw metrics
rf_metrics_sel <- rf_preds_sel %>%
  metrics(truth = truth, estimate = .pred_class)

# Format for display
rf_metrics_pretty <- rf_metrics_sel %>%
  mutate(
    Metric = case_when(
      .metric == "accuracy" ~ "Accuracy",
      .metric == "kap" ~ "Cohen's Kappa"
    ),
    Value = scales::percent(.estimate, accuracy = 0.1)
  ) %>%
  select(Metric, Value)

# Display with gt
rf_metrics_pretty %>%
  gt() %>%
  tab_header(
    title = md("**Table 4. Random Forest Model Performance**")
  )

# ROC AUC for RF
rf_roc <- roc_curve(rf_preds_sel, truth = truth, .pred_High) %>%
  mutate(model = "Random Forest")

rf_auc <- rf_preds_sel %>%
  roc_auc(truth = truth, .pred_High) %>%
  pull(.estimate)
```

The random forest classifier outperformed the logistic regression model, with an accuracy of 87.9% and a Cohen’s Kappa of 70.1%, indicating strong agreement between predictions and true labels. The ROC AUC for this model was `r scales::percent(rf_auc, accuracy = 0.1)`, reflecting stronger discriminative power than the logistic model.

```{r figure7, echo=FALSE}
rf_preds_sel %>%
  conf_mat(truth = truth, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  ggtitle("Figure 7: Confusion Matrix for Random Forest") +
  theme_minimal()
```

The confusion matrix shows improved classification across both groups:

-   True Negatives (Low risk correctly classified): 825

-   False Positives (Low risk misclassified as high): 95

-   False Negatives (High risk misclassified as low): 56

-   True Positives (High risk correctly classified): 275

This model shows higher sensitivity and specificity than logistic regression, with fewer false classifications overall. Particularly, it reduced false negatives — a critical improvement in public health surveillance of HIV risk.

```{r figure8, echo=FALSE}
rf_fit_sel %>%
  extract_fit_parsnip() %>%
  vip::vip(num_features = 10, aesthetics = list(fill = "#F28E2B")) +
  ggtitle("Figure 8: Random Forest Feature Importance") +
  theme_minimal()
```

The top predictors of high HIV risk include:

-   Number of sexual partners

-   Drug use

-   Transactional sex

-   Recent STI history

-   Condom use frequency

These align with established risk factors and confirm that the model is not only accurate but also clinically sensible.

## Tuned Random Forest Model

```{r tuned-rf-model, echo=FALSE}
# Defining a tunable model
rf_model_tuned <- rand_forest(
  mode = "classification",
  trees = 500,
  mtry = tune(),
  min_n = tune()
) %>%
  set_engine("ranger", importance = "impurity")

# Setting up a tuning grid
rf_grid <- grid_regular(
  mtry(range = c(1, length(selected_predictors))),
  min_n(range = c(2, 10)),
  levels = 5
)

# Cross-validation setup
set.seed(2025)
rf_folds <- vfold_cv(hiv_train_sel, v = 5, strata = hiv_risk)

# Workflow
rf_wf_tuned <- workflow() %>%
  add_model(rf_model_tuned) %>%
  add_recipe(hiv_recipe_sel)

# Tuning the model
set.seed(2025)
rf_tune_results <- tune_grid(
  rf_wf_tuned,
  resamples = rf_folds,
  grid = rf_grid,
  metrics = metric_set(accuracy, roc_auc)
)

# Visualizing tuning results
# autoplot(rf_tune_results)
```

```{r best-rf, echo=FALSE}
# Select best parameters based on ROC AUC
best_rf <- rf_tune_results %>%
  select_best(metric = "roc_auc")

# Get performance for the selected config
best_rf_perf <- rf_tune_results %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  inner_join(best_rf, by = c("mtry", "min_n"))

# Format and display
best_rf_perf %>%
  mutate(`Mean ROC AUC` = round(mean, 3)) %>%
  select(
    `Features per Split` = mtry,
    `Minimum Node Size` = min_n,
    `Mean ROC AUC`
  ) %>%
  gt() %>%
  tab_header(title = md("**Table 5. Best Tuning Parameters for Tuned Random Forest Model**"))
```

Optimal hyperparameters were selected through grid search and 5-fold cross-validation. The best-performing configuration involved:

-   4 features per split (`mtry = 4`)

-   Minimum node size of 10 (`min_n = 10`)

-   A mean ROC AUC of 0.942, indicating excellent discriminatory power.

These parameters strike a balance between model complexity and overfitting, yielding robust performance across folds.

```{r final-rf, echo=FALSE}
# Finalize and fit
rf_final <- finalize_workflow(
  rf_wf_tuned,
  best_rf
)

rf_final_fit <- fit(rf_final, data = hiv_train_sel)

# Predict and evaluate
rf_final_preds <- predict(rf_final_fit, hiv_test_sel, type = "prob") %>%
  bind_cols(predict(rf_final_fit, hiv_test_sel), truth = hiv_test_sel$hiv_risk)

rf_final_metrics <- rf_final_preds %>%
  metrics(truth = truth, estimate = .pred_class)

# Pretty version
rf_final_metrics_pretty <- rf_final_metrics %>%
  mutate(
    Metric = case_when(
      .metric == "accuracy" ~ "Accuracy",
      .metric == "kap" ~ "Cohen's Kappa"
    ),
    Value = scales::percent(.estimate, accuracy = 0.1)
  ) %>%
  select(Metric, Value)

# Display
rf_final_metrics_pretty %>%
  gt() %>%
  tab_header(title = md("**Table 6. Tuned Random Forest Model Performance**"))

# ROC AUC
rf_final_auc <- rf_final_preds %>%
  roc_auc(truth = truth, .pred_High) %>%
  pull(.estimate)
```

After tuning, the Random Forest model:

-   Achieved 85.7% accuracy — a slight tradeoff from 87.9% — reflects improved generalizability and reduced overfitting due to tuning.

-   Reached a Cohen’s Kappa of 64.2%, indicating strong agreement between predicted and true risk classifications.

-   Demonstrated the highest discriminative performance, with a test set ROC AUC of `r scales::percent(rf_final_auc, accuracy = 0.1)`, outperforming both the Untuned Random Forest and Logistic Regression models.

-   Showed strong overall effectiveness in identifying individuals at elevated HIV risk based on behavioral and demographic factors.

```{r figure9, echo=FALSE}
rf_final_preds %>%
  conf_mat(truth = truth, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(title = "Figure 9: Confusion Matrix for Tuned Random Forest Model") +
  theme_minimal()
```

The confusion matrix highlights the model’s performance across actual and predicted classes:

-   High-risk individuals (n = 325):
    -   254 correctly identified (true positives)
    -   71 misclassified as low risk (false negatives) → Sensitivity ≈ 78.2%
-   Low-risk individuals (n = 926):
    -   817 correctly identified (true negatives)
    -   109 misclassified as high risk (false positives) → Specificity ≈ 88.3%

The model shows strong balance, favoring minimal false negatives—a key priority in public health screening.

```{r figure10, echo=FALSE}
# Extract the final fitted ranger model
rf_final_fit_model <- extract_fit_parsnip(rf_final_fit)$fit

# Plot variable importance
vip::vip(rf_final_fit_model, num_features = 10, aesthetics = list(fill = "#F28E2B")) +
  labs(title = "Figure 10: Variable Importance for Tuned Random Forest Model") +
  theme_minimal()
```

The top predictors driving model decisions, based on impurity-based importance, were:

1.  Drug Use

2.  Number of Partners

3.  Transactional Sex

4.  Recent STI

5.  Condom Use

These findings are consistent with prior domain knowledge and EDA results, reaffirming that behavioral risk factors are stronger predictors of HIV risk than sociodemographic variables. Notably, PrEP awareness, while important conceptually, played a minimal role in predictive performance within this dataset.

## Model Comparison

```{r model-comparison1, echo=FALSE}
tibble(
  Model = c("Logistic Regression", "Random Forest", "Tuned Random Forest"),
  Accuracy = c(
    log_metrics_sel %>% filter(.metric == "accuracy") %>% pull(.estimate),
    rf_metrics_sel %>% filter(.metric == "accuracy") %>% pull(.estimate),
    rf_final_metrics %>% filter(.metric == "accuracy") %>% pull(.estimate)
  ),
  Kappa = c(
    log_metrics_sel %>% filter(.metric == "kap") %>% pull(.estimate),
    rf_metrics_sel %>% filter(.metric == "kap") %>% pull(.estimate),
    rf_final_metrics %>% filter(.metric == "kap") %>% pull(.estimate)
  ),
  AUC = c(log_auc, rf_auc, rf_final_auc)
) %>%
  mutate(
    Accuracy = scales::percent(Accuracy, accuracy = 0.1),
    Kappa = scales::percent(Kappa, accuracy = 0.1),
    AUC = scales::percent(AUC, accuracy = 0.1)
  ) %>%
  gt() %>%
  tab_header(
    title = md("**Table 7. Model Performance Comparison**")
  ) %>%
  cols_label(
    Model = "Model",
    Accuracy = "Accuracy",
    Kappa = "Cohen’s Kappa",
    AUC = "ROC AUC"
  )
```

All models were evaluated on the same held-out test set, ensuring fair comparison.

The Tuned Random Forest model achieved the highest accuracy (86.1%) and Cohen’s Kappa (64.9%), indicating the strongest agreement between predicted and true HIV risk classifications.

While the Untuned Random Forest performed similarly (85.9% accuracy, 63.8% Kappa), hyperparameter tuning slightly improved performance. Logistic Regression remained a viable baseline (80.7% accuracy) but lagged in agreement (50.2% Kappa), reflecting its limited ability to capture complex, nonlinear patterns.

```{r model-comparison2, echo=FALSE}
# Logistic regression ROC
log_roc <- roc_curve(log_preds_sel, truth = truth, .pred_High) %>%
  mutate(model = "Logistic Regression")

# Untuned random forest ROC
rf_roc <- roc_curve(rf_preds_sel, truth = truth, .pred_High) %>%
  mutate(model = "Random Forest")

# Tuned random forest ROC
rf_final_roc <- roc_curve(rf_final_preds, truth = truth, .pred_High) %>%
  mutate(model = "Tuned Random Forest")

# Combine all ROC curves into one data frame
all_rocs <- bind_rows(log_roc, rf_roc, rf_final_roc)

# Combined ROC plot
ggplot(all_rocs, aes(x = 1 - specificity, y = sensitivity, color = model)) +
  geom_line(linewidth = 1.2) +
  geom_abline(linetype = "dashed", color = "gray") +
  scale_color_manual(values = c(
    "Logistic Regression" = "#4E79A7",
    "Random Forest" = "#59A14F",
    "Tuned Random Forest" = "#F28E2B"
  )) +
  labs(
    title = "Figure 11: ROC Curve Comparison Across Models",
    x = "1 - Specificity",
    y = "Sensitivity",
    color = "Model"
  ) +
  theme_minimal(base_size = 13) +
  theme(legend.position = "top")
```

This ROC curve visualizes and compares the classification performance of the three models across all decision thresholds:

-   Tuned Random Forest (orange) shows the strongest performance overall, maintaining the greatest distance from the diagonal (chance line), particularly at high sensitivity and low false positive rates. → This indicates the best balance between true positives and false positives.

-   Untuned Random Forest (green) also performs well but slightly underperforms the tuned version, especially in the mid-sensitivity range. → Tuning improves model calibration and discrimination.

-   Logistic Regression (blue) demonstrates the lowest ROC AUC among the three models, reflecting weaker discriminative performance. → While still effective, its linear structure limits its ability to capture nonlinear interactions between behavioral predictors.

# Summary of Findings

This project explored the application of machine learning methods to predict HIV risk using a simulated dataset of 5,000 individuals. After cleaning, exploratory analysis, and predictor selection, three classification models were developed:

-   Logistic Regression

-   Untuned Random Forest

-   Tuned Random Forest (via cross-validated hyperparameter tuning)

Key findings:

-   Behavioral risk factors such as transactional sex, drug use, number of partners, condom use, and STI history were the strongest predictors of HIV risk. Sociodemographic variables contributed minimally to model performance.

-   The Tuned Random Forest model outperformed all others, achieving the highest ROC AUC (`r scales::percent(rf_final_auc, accuracy = 0.1)`), along with strong accuracy (85.7%) and balanced sensitivity and specificity.

-   The Untuned Random Forest also performed well (87.9% accuracy), but tuning improved calibration and generalizability by reducing overfitting.

-   Logistic Regression, while interpretable and decently accurate (83.5%), was less capable of capturing nonlinear interactions between predictors.

The results demonstrate that tree-based models, especially when tuned, are well-suited for public health classification tasks involving behavioral data.

# Limitations and Next Steps

Despite the strong performance of the models, several limitations are worth noting:

1.  **Simulated Data**

-   The dataset was synthetically generated, which limits the generalizability of findings to real-world populations.

-   Patterns observed may not reflect actual behavioral distributions or prevalence rates in HIV surveillance data.

2.  **Limited Feature Engineering**

-   Feature creation was limited to existing variables; in real applications, temporal features, interaction terms, or longitudinal behavior tracking might further improve performance.

3.  **No Cost-Sensitive Optimization**

-   All models optimized for overall accuracy and ROC AUC. In public health settings, minimizing false negatives may be more critical. Future work could explore cost-sensitive learning or threshold adjustment to prioritize sensitivity.

4.  **Binary Outcome**

-   The binary HIV risk outcome (low vs. high) simplifies a continuum of risk. More granular outcomes (e.g., risk scores or tiers) might allow for better intervention targeting.

5.  **Interpretability vs. Performance Tradeoff**

-   While Random Forests provided higher accuracy, they sacrifice interpretability. Bridging this gap using model explainability techniques (e.g., SHAP values, partial dependence plots) could enhance trust and adoption in clinical settings.

# Reproducibility

All analysis was conducted in R using the `tidymodels` ecosystem and rendered with `Quarto`. The project is fully reproducible and available upon request.
